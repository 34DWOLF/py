import pandas as pd
import requests
import json
import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer
from transformers import DataCollatorForTokenClassification
from torch.utils.data import Dataset
import numpy as np
import re
import warnings
warnings.filterwarnings('ignore')

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

class NERDataset(Dataset):
    """Dataset class for BERT NER training"""
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        labels = self.labels[idx]
        
        # Tokenize
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_offsets_mapping=True,
            return_tensors='pt'
        )
        
        # Create label array
        label_array = np.ones(self.max_length, dtype=int) * -100  # -100 is ignored by loss
        
        # Map labels to tokens
        offset_mapping = encoding['offset_mapping'][0].numpy()
        
        for start, end, label_id in labels:
            for idx, (token_start, token_end) in enumerate(offset_mapping):
                if token_start >= start and token_end <= end and token_start != token_end:
                    label_array[idx] = label_id
        
        encoding['labels'] = torch.tensor(label_array)
        encoding['input_ids'] = encoding['input_ids'].squeeze()
        encoding['attention_mask'] = encoding['attention_mask'].squeeze()
        
        return {k: v for k, v in encoding.items() if k != 'offset_mapping'}

def fetch_mitre_attack_data():
    """
    Fetch MITRE ATT&CK data from GitHub
    """
    print("Fetching MITRE ATT&CK data...")
    
    # MITRE ATT&CK STIX data URL
    url = "https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json"
    
    try:
        response = requests.get(url)
        response.raise_for_status()
        data = response.json()
        
        # Extract threat actors (intrusion-sets)
        threat_actors = []
        for obj in data['objects']:
            if obj['type'] == 'intrusion-set':
                actor_info = {
                    'Name': obj.get('name', ''),
                    'Description': obj.get('description', ''),
                    'Aliases': obj.get('aliases', []),
                    'Created': obj.get('created', ''),
                    'Modified': obj.get('modified', '')
                }
                threat_actors.append(actor_info)
        
        mitre_df = pd.DataFrame(threat_actors)
        print(f"Successfully fetched {len(mitre_df)} threat actors from MITRE ATT&CK")
        return mitre_df
    
    except Exception as e:
        print(f"Error fetching MITRE data: {e}")
        return pd.DataFrame()

def create_bert_ner_model(df_spacy_train):
    """
    Create and train a BERT NER model for nation state extraction
    """
    print("\nCreating BERT NER model...")
    
    # Load pre-trained BERT model and tokenizer
    model_name = "bert-base-uncased"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # Label mappings
    label2id = {"O": 0, "B-NATION": 1, "I-NATION": 2}
    id2label = {v: k for k, v in label2id.items()}
    
    # Load model
    model = AutoModelForTokenClassification.from_pretrained(
        model_name,
        num_labels=len(label2id),
        label2id=label2id,
        id2label=id2label
    ).to(device)
    
    # Prepare training data
    print(f"Processing {len(df_spacy_train)} training examples...")
    texts = []
    labels = []
    
    # Country name variations
    country_variations = {
        'Russia': ['Russia', 'Russian', 'Russian Federation', 'USSR', 'Soviet'],
        'China': ['China', 'Chinese', 'PRC', "People's Republic of China"],
        'Iran': ['Iran', 'Iranian', 'Persia', 'Persian'],
        'North Korea': ['North Korea', 'North Korean', 'DPRK'],
        'United States': ['United States', 'American', 'USA', 'US', 'U.S.'],
        'Israel': ['Israel', 'Israeli'],
        'India': ['India', 'Indian'],
        'Pakistan': ['Pakistan', 'Pakistani'],
        'Vietnam': ['Vietnam', 'Vietnamese'],
        'Lebanon': ['Lebanon', 'Lebanese'],
        'South Korea': ['South Korea', 'South Korean', 'ROK', 'Republic of Korea'],
        'United Kingdom': ['United Kingdom', 'UK', 'British', 'Britain'],
        'Ukraine': ['Ukraine', 'Ukrainian'],
        'Belarus': ['Belarus', 'Belarusian'],
    }
    
    for _, row in df_spacy_train.iterrows():
        text = str(row.get('Description', ''))
        country = str(row.get('Suspect', ''))
        
        if not text or not country:
            continue
        
        # Find entities in text
        entities = []
        variations = country_variations.get(country, [country])
        
        for variation in variations:
            for match in re.finditer(re.escape(variation), text, re.IGNORECASE):
                start_idx = match.start()
                end_idx = match.end()
                # Use B-NATION for beginning, I-NATION for inside
                entities.append((start_idx, end_idx, 1))  # 1 is B-NATION
        
        if entities:
            # Remove duplicates and sort
            entities = list(set(entities))
            entities.sort(key=lambda x: x[0])
            
            # Remove overlapping entities
            filtered_entities = []
            last_end = -1
            for start, end, label in entities:
                if start >= last_end:
                    filtered_entities.append((start, end, label))
                    last_end = end
            
            texts.append(text)
            labels.append(filtered_entities)
    
    if not texts:
        raise ValueError("No valid training data found. Please ensure df_spacy_train contains valid data.")
    
    print(f"Created {len(texts)} training examples")
    
    # Create dataset
    train_dataset = NERDataset(texts, labels, tokenizer)
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir="./bert_ner_model",
        num_train_epochs=3,
        per_device_train_batch_size=8,
        warmup_steps=100,
        weight_decay=0.01,
        logging_steps=10,
        save_steps=1000,
        eval_steps=1000,
        save_total_limit=2,
        load_best_model_at_end=False,
        push_to_hub=False,
        report_to="none",  # Disable wandb/tensorboard
    )
    
    # Data collator
    data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8)
    
    # Create trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
    )
    
    # Train the model
    print("Training BERT model...")
    trainer.train()
    
    print("BERT model trained successfully")
    return model, tokenizer

def extract_nation_states_with_bert(mitre_df, model, tokenizer):
    """
    Extract nation states from MITRE descriptions using trained BERT NER model
    """
    print("\nExtracting nation states from MITRE descriptions using BERT...")
    
    # Create a copy to store results
    result_df = mitre_df.copy()
    result_df['primary_nation_state'] = ''
    result_df['match_source'] = ''
    
    extracted_count = 0
    
    # Nation mapping for normalization
    nation_mapping = {
        'russian': 'Russia', 'russia': 'Russia', 'russian federation': 'Russia', 'ussr': 'Russia', 'soviet': 'Russia',
        'chinese': 'China', 'china': 'China', 'prc': 'China',
        'iranian': 'Iran', 'iran': 'Iran', 'persia': 'Iran', 'persian': 'Iran',
        'north korean': 'North Korea', 'north korea': 'North Korea', 'dprk': 'North Korea',
        'american': 'United States', 'united states': 'United States', 'usa': 'United States', 'us': 'United States', 'u.s.': 'United States',
        'israeli': 'Israel', 'israel': 'Israel',
        'indian': 'India', 'india': 'India',
        'pakistani': 'Pakistan', 'pakistan': 'Pakistan',
        'vietnamese': 'Vietnam', 'vietnam': 'Vietnam',
        'lebanese': 'Lebanon', 'lebanon': 'Lebanon',
        'south korean': 'South Korea', 'south korea': 'South Korea', 'rok': 'South Korea',
        'ukrainian': 'Ukraine', 'ukraine': 'Ukraine',
        'belarusian': 'Belarus', 'belarus': 'Belarus',
        'british': 'United Kingdom', 'united kingdom': 'United Kingdom', 'uk': 'United Kingdom',
    }
    
    for idx, row in result_df.iterrows():
        # Combine name, description and aliases for context
        name = row['Name']
        description = row['Description']
        aliases = row['Aliases']
        
        # Create text for analysis
        text_parts = []
        if name:
            text_parts.append(str(name))
        if description:
            text_parts.append(str(description))
        if isinstance(aliases, list) and aliases:
            text_parts.extend([str(alias) for alias in aliases])
        
        combined_text = ' '.join(text_parts)
        
        # Tokenize and predict
        inputs = tokenizer(
            combined_text,
            truncation=True,
            max_length=512,
            return_tensors="pt"
        ).to(device)
        
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.argmax(outputs.logits, dim=-1)
        
        # Extract entities
        tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
        predictions = predictions[0].cpu().numpy()
        
        current_entity = []
        entities = []
        
        for token, pred in zip(tokens, predictions):
            if pred == 1:  # B-NATION
                if current_entity:
                    entities.append(' '.join(current_entity))
                current_entity = [token.replace('##', '')]
            elif pred == 2 and current_entity:  # I-NATION
                current_entity.append(token.replace('##', ''))
            else:
                if current_entity:
                    entities.append(' '.join(current_entity))
                    current_entity = []
        
        if current_entity:
            entities.append(' '.join(current_entity))
        
        # Normalize and select nation
        if entities:
            # Normalize the first entity found
            nation_text = entities[0].lower().strip()
            nation = nation_mapping.get(nation_text, entities[0])
            
            result_df.at[idx, 'primary_nation_state'] = nation
            result_df.at[idx, 'match_source'] = 'bert_extraction'
            extracted_count += 1
    
    print(f"Extracted nation states for {extracted_count} entries using BERT")
    return result_df

def match_threat_actors(result_df, raw_df):
    """
    Match MITRE threat actors with raw_df based on threat actor names
    Only processes entries that don't already have a nation state
    """
    print("\nMatching remaining threat actors with raw_df...")
    
    # Normalize names for better matching
    def normalize_name(name):
        if pd.isna(name):
            return ''
        # Remove special characters and extra spaces
        name = str(name).strip().lower()
        name = re.sub(r'[^\w\s-]', '', name)
        return name
    
    # Create normalized columns for matching
    result_df['name_normalized'] = result_df['Name'].apply(normalize_name)
    raw_df['threat_actor_normalized'] = raw_df['Threat actor name'].apply(normalize_name)
    
    # Only process entries without nation states
    no_nation_mask = result_df['primary_nation_state'] == ''
    matched_count = 0
    
    for idx in result_df[no_nation_mask].index:
        mitre_name = result_df.at[idx, 'name_normalized']
        
        # Check direct match
        match = raw_df[raw_df['threat_actor_normalized'] == mitre_name]
        if not match.empty:
            nation = match.iloc[0]['primary_nation_state']
            if pd.notna(nation) and nation.strip():
                result_df.at[idx, 'primary_nation_state'] = nation
                result_df.at[idx, 'match_source'] = 'threat_actors_exact'
                matched_count += 1
                continue
        
        # Check if MITRE name is in raw_df threat actor name (partial match)
        for _, raw_row in raw_df.iterrows():
            if mitre_name and (mitre_name in raw_row['threat_actor_normalized'] or \
               raw_row['threat_actor_normalized'] in mitre_name):
                nation = raw_row['primary_nation_state']
                if pd.notna(nation) and nation.strip():
                    result_df.at[idx, 'primary_nation_state'] = nation
                    result_df.at[idx, 'match_source'] = 'threat_actors_partial'
                    matched_count += 1
                    break
        
        # Check aliases
        if result_df.at[idx, 'primary_nation_state'] == '':
            aliases = result_df.at[idx, 'Aliases']
            if isinstance(aliases, list):
                for alias in aliases:
                    alias_normalized = normalize_name(alias)
                    match = raw_df[raw_df['threat_actor_normalized'] == alias_normalized]
                    if not match.empty:
                        nation = match.iloc[0]['primary_nation_state']
                        if pd.notna(nation) and nation.strip():
                            result_df.at[idx, 'primary_nation_state'] = nation
                            result_df.at[idx, 'match_source'] = 'threat_actors_alias'
                            matched_count += 1
                            break
    
    # Clean up temporary columns
    result_df = result_df.drop(['name_normalized'], axis=1)
    
    print(f"Matched {matched_count} additional threat actors from raw_df")
    
    return result_df

def main(raw_df, df_spacy_train):
    """
    Main function to orchestrate the entire process
    Requires df_spacy_train with columns: 'Description' and 'Suspect'
    """
    print("Starting MITRE ATT&CK threat actor matching process...")
    
    # Step 1: Fetch MITRE ATT&CK data
    mitre_df = fetch_mitre_attack_data()
    if mitre_df.empty:
        print("Failed to fetch MITRE data. Exiting.")
        return pd.DataFrame()
    
    # Step 2: Train BERT model with provided training data
    try:
        model, tokenizer = create_bert_ner_model(df_spacy_train)
    except ValueError as e:
        print(f"Error: {e}")
        return pd.DataFrame()
    
    # Step 3: Extract nation states from MITRE descriptions using BERT
    result_df = extract_nation_states_with_bert(mitre_df, model, tokenizer)
    
    # Step 4: Match remaining actors with raw_df
    final_df = match_threat_actors(result_df, raw_df)
    
    # Summary statistics
    print("\n=== Summary ===")
    print(f"Total threat actors: {len(final_df)}")
    print(f"Matched nation states: {(final_df['primary_nation_state'] != '').sum()}")
    print(f"Blank nation states: {(final_df['primary_nation_state'] == '').sum()}")
    
    # Print breakdown by source
    print("\n=== Match Source Breakdown ===")
    source_counts = final_df['match_source'].value_counts()
    for source, count in source_counts.items():
        if source:
            print(f"{source}: {count}")
    
    # Print all results with full details
    print("\n=== Full Results ===")
    # Prepare display dataframe
    display_df = final_df[['Name', 'primary_nation_state', 'match_source', 'Description']].copy()
    display_df['primary_nation_state'] = display_df['primary_nation_state'].fillna('')
    display_df['match_source'] = display_df['match_source'].fillna('')
    
    # Truncate description for display
    display_df['Description'] = display_df['Description'].apply(
        lambda x: (str(x)[:100] + '...') if pd.notna(x) and len(str(x)) > 100 else str(x)
    )
    
    # Set pandas display options for better output
    pd.set_option('display.max_rows', None)
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    pd.set_option('display.max_colwidth', None)
    
    print(display_df.to_string(index=False))
    
    # Reset display options
    pd.reset_option('display.max_rows')
    pd.reset_option('display.max_columns')
    pd.reset_option('display.width')
    pd.reset_option('display.max_colwidth')
    
    return final_df

# Execute the MITRE ATT&CK threat actor matching
if __name__ == "__main__":
    # Load training data from SQL
    # df_spacy_train = pd.read_sql(
    #     'SELECT "Description", "Suspect" FROM threat_actor_training',
    #     connection
    # )
    
    # For testing, use sample training data
    df_spacy_train = pd.DataFrame({
        'Description': [
            'APT28 is a threat group that has been attributed to Russia\'s General Staff Main Intelligence Directorate',
            'Lazarus Group is a North Korean state-sponsored cyber threat group',
            'APT1 is a Chinese cyber espionage group',
            'Indrik Spider is a Russia-based cybercriminal group',
        ],
        'Suspect': [
            'Russia',
            'North Korea',
            'China',
            'Russia'
        ]
    })
    
    # Run the main function with required training data
    result_df = main(raw_df, df_spacy_train)
