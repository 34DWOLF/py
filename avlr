# Import required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller, acf, pacf
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error
from datetime import datetime, timedelta
import warnings
from dateutil.relativedelta import relativedelta
import logging

# ====================================================
# PARAMETER SECTION - Modify these values as needed
# ====================================================
# Minimum number of months required for ARIMA modeling
MIN_MONTHS_FOR_ARIMA = 24  # At least 2 years of data required for ARIMA

# Minimum number of months required for stationarity testing
MIN_MONTHS_FOR_STATIONARITY = 8

# Number of months to forecast
FORECAST_MONTHS = 12

# Maximum lags to consider for ACF/PACF analysis
MAX_LAGS_DEFAULT = 10
MAX_LAGS_SEASONAL = 24

# Confidence interval level (%)
CONFIDENCE_LEVEL = 95
Z_VALUE = 1.96  # Corresponds to 95% confidence

# Reconciliation method
# Options: "proportional", "weight_forecast", "weight_history"
RECONCILIATION_METHOD = "proportional"  

# Plot settings
FIGSIZE_TIMESERIES = (14, 7)
FIGSIZE_ACF_PACF = (12, 8)
# ====================================================

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('forecast_model')

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

# Set plot style for better visualizations
plt.style.use('seaborn-whitegrid')

# Load data (replace with your actual file path in Databricks)
file_path = "/dbfs/FileStore/your_data_file.csv"  # Update this path
logger.info(f"Loading data from {file_path}")
df = pd.read_csv(file_path)

# Ensure Date is in datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Since data is monthly, ensure it's set to the first day of each month for consistency
df['Date'] = df['Date'].apply(lambda x: x.replace(day=1))
logger.info(f"Data loaded successfully with {len(df)} rows")

# Display the first few rows
print("Data sample:")
display(df.head())

# Get basic statistics
print("\nData summary:")
display(df.describe())

# Check for missing values
print("\nMissing values:")
display(df.isnull().sum())

# Get unique owners
all_owners = df['Owner'].unique()
logger.info(f"Found {len(all_owners)} unique owners")

# Separate "Total" from other owners
is_total_present = "Total" in all_owners
if is_total_present:
    other_owners = [owner for owner in all_owners if owner != "Total"]
    logger.info(f"Found 'Total' owner and {len(other_owners)} other owners")
else:
    other_owners = all_owners
    logger.info("No 'Total' owner found, treating all owners individually")

print(f"\nNumber of unique owners: {len(all_owners)}")
print("Owners:", all_owners)

# Count data points per owner
owner_counts = df.groupby('Owner').size().reset_index(name='Count')
print("\nData points per owner:")
display(owner_counts)

# Helper function to convert numpy.datetime64 to Python datetime
def to_python_datetime(dt):
    if isinstance(dt, np.datetime64):
        return pd.Timestamp(dt).to_pydatetime()
    return dt

# Check date ranges per owner
date_ranges = []
for owner in all_owners:
    owner_data = df[df['Owner'] == owner].sort_values('Date')
    first_date = to_python_datetime(owner_data['Date'].min())
    last_date = to_python_datetime(owner_data['Date'].max())
    
    # Calculate months difference properly
    months_diff = (last_date.year - first_date.year) * 12 + last_date.month - first_date.month + 1
    
    date_ranges.append({
        'Owner': owner,
        'First Month': first_date.strftime('%Y-%m'),
        'Last Month': last_date.strftime('%Y-%m'),
        'Months Count': len(owner_data),
        'Expected Months': months_diff,
        'Is Continuous': len(owner_data) == months_diff
    })

date_range_df = pd.DataFrame(date_ranges)
print("\nDate ranges per owner:")
display(date_range_df)

# Define stationarity check function
def check_stationarity(timeseries, owner_name=None):
    """
    Test stationarity using Augmented Dickey-Fuller test
    """
    # Perform ADF test
    result = adfuller(timeseries)
    
    # Extract results
    dftest = pd.Series(
        result[0:4],
        index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used']
    )
    
    for key, value in result[4].items():
        dftest[f'Critical Value ({key})'] = value
    
    # Determine if stationary
    is_stationary = result[1] <= 0.05
    
    # Display results
    if owner_name:
        print(f"Results of Augmented Dickey-Fuller Test for Owner: {owner_name}")
    else:
        print("Results of Augmented Dickey-Fuller Test:")
    
    print(dftest)
    print(f"Is the series stationary? {'Yes' if is_stationary else 'No'}")
    print("-" * 50)
    
    return is_stationary, result[1]

# Plot time series for each owner
logger.info("Plotting time series for each owner")
plt.figure(figsize=(FIGSIZE_TIMESERIES[0], len(all_owners) * 4))

for i, owner in enumerate(all_owners):
    owner_data = df[df['Owner'] == owner].sort_values('Date')
    
    plt.subplot(len(all_owners), 1, i+1)
    plt.plot(owner_data['Date'], owner_data['Value'], marker='o', linestyle='-', label=f"Data Points: {len(owner_data)}")
    plt.title(f'Monthly Time Series for Owner: {owner}')
    plt.ylabel('Value')
    plt.grid(True)
    plt.legend()
    
    # Format x-axis to show month-year
    plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%b %Y'))
    plt.gcf().autofmt_xdate()

plt.tight_layout()
display(plt.gcf())
plt.close()

# Test stationarity for each owner
logger.info("Testing stationarity for each owner")
stationarity_results = {}

for owner in all_owners:
    owner_data = df[df['Owner'] == owner].sort_values('Date')
    values = owner_data['Value'].values
    
    # Only test if we have enough data points
    if len(values) >= MIN_MONTHS_FOR_STATIONARITY:
        logger.info(f"Testing stationarity for owner: {owner}")
        is_stationary, p_value = check_stationarity(values, owner)
        stationarity_results[owner] = {
            'is_stationary': is_stationary,
            'p_value': p_value,
            'data_points': len(values)
        }
    else:
        logger.warning(f"Owner {owner} has only {len(values)} months of data, not enough for stationarity test.")
        print(f"Owner {owner} has only {len(values)} months of data, not enough for stationarity test.")
        stationarity_results[owner] = {
            'is_stationary': None,
            'p_value': None,
            'data_points': len(values)
        }

# Function to analyze and plot ACF and PACF
def analyze_acf_pacf(timeseries, owner_name, max_lags=MAX_LAGS_DEFAULT):
    """
    Analyze and plot ACF and PACF to determine ARIMA order
    """
    logger.info(f"Analyzing ACF/PACF for owner: {owner_name}, max_lags: {max_lags}")
    
    # Compute ACF and PACF
    acf_values = acf(timeseries, nlags=max_lags, fft=True)
    pacf_values = pacf(timeseries, nlags=max_lags, method='ols')
    
    # Plot
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=FIGSIZE_ACF_PACF)
    
    # ACF plot
    plot_acf(timeseries, ax=ax1, lags=max_lags)
    ax1.set_title(f'Autocorrelation Function for Owner: {owner_name}')
    
    # PACF plot
    plot_pacf(timeseries, ax=ax2, lags=max_lags)
    ax2.set_title(f'Partial Autocorrelation Function for Owner: {owner_name}')
    
    plt.tight_layout()
    display(plt.gcf())
    plt.close()
    
    # Find significant lags (95% confidence)
    significance_level = 1.96/np.sqrt(len(timeseries))
    
    # Significant lags in ACF (for MA/q order)
    significant_acf = [i for i in range(1, max_lags + 1) if abs(acf_values[i]) > significance_level]
    
    # Significant lags in PACF (for AR/p order)
    significant_pacf = [i for i in range(1, max_lags + 1) if abs(pacf_values[i]) > significance_level]
    
    print(f"Owner: {owner_name}")
    print(f"Significant lags in ACF (suggests MA/q order): {significant_acf}")
    print(f"Significant lags in PACF (suggests AR/p order): {significant_pacf}")
    
    # Check for seasonality (12 months)
    if max_lags >= 12 and len(timeseries) >= 24:
        seasonal_lag = 12
        if seasonal_lag in significant_acf or seasonal_lag in significant_pacf:
            logger.info(f"Potential yearly seasonality detected at lag {seasonal_lag} for {owner_name}")
            print(f"Potential yearly seasonality detected at lag {seasonal_lag}")
    
    print("-" * 50)
    
    # Suggested order
    p = max(significant_pacf) if significant_pacf else 1
    q = max(significant_acf) if significant_acf else 1
    
    return p, q, acf_values, pacf_values

# Analyze ACF and PACF for each owner with sufficient data
logger.info("Analyzing ACF/PACF for each owner")
acf_pacf_results = {}

for owner in all_owners:
    owner_data = df[df['Owner'] == owner].sort_values('Date')
    values = owner_data['Value'].values
    
    # Only analyze if we have enough data points
    if len(values) >= MIN_MONTHS_FOR_ARIMA:
        print(f"Analyzing ACF/PACF for Owner: {owner} (Months of data: {len(values)})")
        
        # Check if differencing is needed
        is_stationary, _ = stationarity_results[owner]['is_stationary'], stationarity_results[owner]['p_value']
        
        # If not stationary, difference the series
        if not is_stationary:
            logger.info(f"Series for {owner} is not stationary, applying differencing")
            print(f"Series for {owner} is not stationary, applying differencing")
            values_diff = np.diff(values)
            p, q, acf_vals, pacf_vals = analyze_acf_pacf(values_diff, f"{owner} (differenced)")
            d = 1
        else:
            p, q, acf_vals, pacf_vals = analyze_acf_pacf(values, owner)
            d = 0
            
        # For monthly data, we might need higher max_lags to catch seasonality
        if len(values) >= 36:  # If we have at least 3 years of data
            logger.info(f"Checking for seasonality with additional lags for {owner}")
            p_seasonal, q_seasonal, _, _ = analyze_acf_pacf(values if is_stationary else values_diff, 
                                                         f"{owner} (seasonal check)", 
                                                         max_lags=MAX_LAGS_SEASONAL)
            # Use the higher of the two if seasonal analysis found something
            p = max(p, p_seasonal) if p_seasonal > p else p
            q = max(q, q_seasonal) if q_seasonal > q else q
            
        acf_pacf_results[owner] = {
            'p': p,
            'd': d,
            'q': q,
            'acf_values': acf_vals,
            'pacf_values': pacf_vals
        }
        
        logger.info(f"Suggested ARIMA order for {owner}: ({p}, {d}, {q})")
        print(f"Suggested ARIMA order for {owner}: ({p}, {d}, {q})")
    else:
        logger.warning(f"Owner {owner} has only {len(values)} months of data, not enough for robust ACF/PACF analysis.")
        print(f"Owner {owner} has only {len(values)} months of data, not enough for robust ACF/PACF analysis.")
        acf_pacf_results[owner] = None

# Function to calculate evaluation metrics
def calculate_metrics(y_true, y_pred):
    """Calculate various evaluation metrics using sklearn"""
    metrics = {}
    
    # R-squared (coefficient of determination)
    metrics['r2'] = r2_score(y_true, y_pred)
    
    # Mean Squared Error
    metrics['mse'] = mean_squared_error(y_true, y_pred)
    
    # Root Mean Squared Error
    metrics['rmse'] = np.sqrt(metrics['mse'])
    
    # Mean Absolute Error
    metrics['mae'] = mean_absolute_error(y_true, y_pred)
    
    # Mean Absolute Percentage Error
    # Handle case where y_true contains zeros
    if np.any(y_true == 0):
        # Calculate MAPE only for non-zero values
        non_zero_indices = y_true != 0
        if np.any(non_zero_indices):
            metrics['mape'] = mean_absolute_percentage_error(
                y_true[non_zero_indices], 
                y_pred[non_zero_indices]
            ) * 100  # Convert to percentage
        else:
            metrics['mape'] = np.nan
    else:
        metrics['mape'] = mean_absolute_percentage_error(y_true, y_pred) * 100  # Convert to percentage
    
    return metrics

# Train ARIMA models for owners with sufficient data
logger.info("Training ARIMA models for eligible owners")
arima_results = {}

for owner in all_owners:
    owner_data = df[df['Owner'] == owner].sort_values('Date')
    values = owner_data['Value'].values
    dates = owner_data['Date'].values
    
    # Only attempt ARIMA if we have enough data points
    if len(values) >= MIN_MONTHS_FOR_ARIMA and acf_pacf_results[owner] is not None:
        logger.info(f"Training ARIMA model for Owner: {owner}")
        print(f"Training ARIMA model for Owner: {owner}")
        
        # Get suggested order
        p, d, q = acf_pacf_results[owner]['p'], acf_pacf_results[owner]['d'], acf_pacf_results[owner]['q']
        
        try:
            # Train ARIMA model
            model = ARIMA(values, order=(p, d, q))
            model_fit = model.fit()
            
            # Print model summary
            print(f"ARIMA model summary for Owner: {owner}")
            print(model_fit.summary())
            
            # Calculate in-sample predictions
            if d > 0:
                # For differenced models, we need to handle predictions differently
                in_sample_pred = model_fit.predict(start=d, end=len(values)-1)
                # Ensure lengths match for metric calculation
                actuals = values[d:]
            else:
                in_sample_pred = model_fit.predict(start=0, end=len(values)-1)
                actuals = values
            
            # Calculate metrics
            metrics = calculate_metrics(actuals, in_sample_pred)
            
            arima_results[owner] = {
                'model_fit': model_fit,
                'order': (p, d, q),
                'values': values,
                'dates': dates,
                'model_type': f"ARIMA({p},{d},{q})",
                'in_sample_pred': in_sample_pred,
                'metrics': metrics
            }
            
            # Print metrics
            print(f"ARIMA model metrics for {owner}:")
            for metric_name, metric_value in metrics.items():
                print(f"  {metric_name.upper()}: {metric_value:.4f}")
            
            logger.info(f"Successfully trained ARIMA({p},{d},{q}) for {owner}")
            
        except Exception as e:
            logger.warning(f"Error training ARIMA for {owner}: {str(e)}")
            print(f"Error training ARIMA for {owner}: {str(e)}")
            print("Trying simpler ARIMA(1,1,1) model...")
            
            try:
                # Try with simpler model
                model = ARIMA(values, order=(1, 1, 1))
                model_fit = model.fit()
                
                # Calculate in-sample predictions
                in_sample_pred = model_fit.predict(start=1, end=len(values)-1)
                
                # Calculate metrics
                metrics = calculate_metrics(values[1:], in_sample_pred)
                
                arima_results[owner] = {
                    'model_fit': model_fit,
                    'order': (1, 1, 1),
                    'values': values,
                    'dates': dates,
                    'model_type': "ARIMA(1,1,1)",
                    'in_sample_pred': in_sample_pred,
                    'metrics': metrics
                }
                
                # Print metrics
                print(f"ARIMA model metrics for {owner}:")
                for metric_name, metric_value in metrics.items():
                    print(f"  {metric_name.upper()}: {metric_value:.4f}")
                
                logger.info(f"Successfully trained simpler ARIMA(1,1,1) for {owner}")
                
            except Exception as e2:
                logger.error(f"Error training simpler ARIMA for {owner}: {str(e2)}")
                print(f"Error training simpler ARIMA for {owner}: {str(e2)}")
                arima_results[owner] = None
    else:
        logger.info(f"Owner {owner} has insufficient data for ARIMA modeling.")
        print(f"Owner {owner} has insufficient data for ARIMA modeling.")
        arima_results[owner] = None

# Train linear regression models for all owners
logger.info("Training Linear Regression models for all owners")
linear_results = {}

for owner in all_owners:
    owner_data = df[df['Owner'] == owner].sort_values('Date')
    values = owner_data['Value'].values
    dates = owner_data['Date'].to_list()  # Convert to Python datetime objects
    
    logger.info(f"Training Linear Regression model for Owner: {owner}")
    print(f"Training Linear Regression model for Owner: {owner}")
    
    # For monthly data, convert dates to numeric feature (months since first date)
    first_date = min(dates)
    # Properly calculate months since first date
    X = np.array([
        (to_python_datetime(d).year - to_python_datetime(first_date).year) * 12 + 
        to_python_datetime(d).month - to_python_datetime(first_date).month 
        for d in dates
    ]).reshape(-1, 1)
    
    # Fit model
    model = LinearRegression()
    model.fit(X, values)
    
    # Generate in-sample predictions
    in_sample_pred = model.predict(X)
    
    # Calculate metrics
    metrics = calculate_metrics(values, in_sample_pred)
    
    # Print coefficient details
    print(f"Linear Regression coefficients for {owner}:")
    print(f"  Intercept: {model.intercept_:.4f}")
    print(f"  Slope (per month): {model.coef_[0]:.4f}")
    
    # Print metrics
    print(f"Linear Regression model metrics for {owner}:")
    for metric_name, metric_value in metrics.items():
        print(f"  {metric_name.upper()}: {metric_value:.4f}")
    
    linear_results[owner] = {
        'model_fit': model,
        'first_date': first_date,
        'values': values,
        'dates': dates,
        'in_sample_pred': in_sample_pred,
        'metrics': metrics,
        'model_type': "Linear Regression"
    }
    logger.info(f"Successfully trained Linear Regression for {owner}, R-squared: {metrics['r2']:.4f}")

# Generate initial forecasts for each owner
logger.info(f"Generating {FORECAST_MONTHS}-month initial forecasts for each owner")
initial_forecast_results = {}

# Store model metadata for MLflow tracking
model_metadata = {}

for owner in all_owners:
    logger.info(f"Generating initial forecast for Owner: {owner}")
    print(f"Generating initial forecast for Owner: {owner}")
    
    # Check if we have a valid ARIMA model
    if owner in arima_results and arima_results[owner] is not None:
        logger.info(f"Using {arima_results[owner]['model_type']} model for {owner}")
        print(f"Using {arima_results[owner]['model_type']} model for {owner}")
        
        # Get the model and data
        model_fit = arima_results[owner]['model_fit']
        values = arima_results[owner]['values']
        dates = arima_results[owner]['dates']
        model_type = arima_results[owner]['model_type']
        metrics = arima_results[owner]['metrics']
        
        # Store model metadata
        model_metadata[owner] = {
            'model_fit': model_fit,
            'model_type': model_type,
            'framework': 'statsmodels.ARIMA',
            'metrics': metrics,
            'params': {
                'order': arima_results[owner]['order']
            }
        }
        
        # Generate forecast
        forecast = model_fit.forecast(steps=FORECAST_MONTHS)
        forecast_conf = model_fit.get_forecast(steps=FORECAST_MONTHS)
        conf_int = forecast_conf.conf_int()
        
        # Generate future dates (monthly)
        last_date = to_python_datetime(dates[-1])
        future_dates = [last_date + relativedelta(months=i+1) for i in range(FORECAST_MONTHS)]
        
        initial_forecast_results[owner] = {
            'forecast': forecast,
            'conf_int': conf_int,
            'future_dates': future_dates,
            'model_type': model_type
        }
    else:
        logger.info(f"Using Linear Regression model for {owner}")
        print(f"Using Linear Regression model for {owner}")
        
        # Get the model and data
        model_fit = linear_results[owner]['model_fit']
        first_date = linear_results[owner]['first_date']
        values = linear_results[owner]['values']
        dates = linear_results[owner]['dates']
        metrics = linear_results[owner]['metrics']
        
        # Store model metadata
        model_metadata[owner] = {
            'model_fit': model_fit,
            'model_type': 'Linear Regression',
            'framework': 'sklearn.LinearRegression',
            'metrics': metrics,
            'params': {
                'fit_intercept': True
            }
        }
        
        # Get the last date
        last_date = to_python_datetime(dates[-1])
        
        # Generate future months (as X values)
        months_since_first = [
            ((last_date.year - to_python_datetime(first_date).year) * 12 + 
             last_date.month - to_python_datetime(first_date).month) + i + 1 
            for i in range(FORECAST_MONTHS)
        ]
        future_X = np.array(months_since_first).reshape(-1, 1)
        
        # Generate forecast
        forecast = model_fit.predict(future_X)
        
        # Generate simple confidence intervals
        # (This is a simplification - not statistically rigorous)
        prediction_std = np.std(values - linear_results[owner]['in_sample_pred'])
        conf_lower = forecast - Z_VALUE * prediction_std
        conf_upper = forecast + Z_VALUE * prediction_std
        
        # Create a DataFrame similar to ARIMA's conf_int
        conf_int = pd.DataFrame({
            'lower': conf_lower,
            'upper': conf_upper
        })
        
        # Generate future dates (monthly)
        future_dates = [last_date + relativedelta(months=i+1) for i in range(FORECAST_MONTHS)]
        
        initial_forecast_results[owner] = {
            'forecast': forecast,
            'conf_int': conf_int,
            'future_dates': future_dates,
            'model_type': "Linear Regression"
        }

# Reconcile forecasts if 'Total' is present
if is_total_present:
    logger.info("Reconciling forecasts to ensure consistency with 'Total'")
    print("\nReconciling forecasts to ensure consistency with 'Total'...")
    
    # Get total forecast
    total_forecast = initial_forecast_results["Total"]["forecast"]
    
    # Get sum of individual forecasts
    individual_forecasts = np.zeros(FORECAST_MONTHS)
    for owner in other_owners:
        individual_forecasts += initial_forecast_results[owner]["forecast"]
    
    # Calculate discrepancy
    discrepancy = total_forecast - individual_forecasts
    print(f"Discrepancy between 'Total' and sum of individuals: {discrepancy}")
    
    # Reconciliation methods
    reconciled_forecast_results = {}
    reconciled_forecast_results["Total"] = initial_forecast_results["Total"]  # Keep original Total
    
    if RECONCILIATION_METHOD == "proportional":
        # Proportional reconciliation
        logger.info("Using proportional reconciliation method")
        print("Using proportional reconciliation method")
        
        for month in range(FORECAST_MONTHS):
            # Skip reconciliation if sum is close to zero to avoid division by zero
            if abs(individual_forecasts[month]) < 1e-10:
                for owner in other_owners:
                    if "reconciled_forecast" not in reconciled_forecast_results.get(owner, {}):
                        reconciled_forecast_results[owner] = {
                            "reconciled_forecast": initial_forecast_results[owner]["forecast"].copy(),
                            "future_dates": initial_forecast_results[owner]["future_dates"],
                            "model_type": initial_forecast_results[owner]["model_type"],
                            "initial_forecast": initial_forecast_results[owner]["forecast"].copy(),
                            "conf_int": initial_forecast_results[owner]["conf_int"]
                        }
                continue
            
            # Calculate proportional adjustment for each owner
            for owner in other_owners:
                if "reconciled_forecast" not in reconciled_forecast_results.get(owner, {}):
                    reconciled_forecast_results[owner] = {
                        "reconciled_forecast": initial_forecast_results[owner]["forecast"].copy(),
                        "future_dates": initial_forecast_results[owner]["future_dates"],
                        "model_type": initial_forecast_results[owner]["model_type"],
                        "initial_forecast": initial_forecast_results[owner]["forecast"].copy(),
                        "conf_int": initial_forecast_results[owner]["conf_int"]
                    }
                
                # Proportional adjustment
                owner_forecast = initial_forecast_results[owner]["forecast"][month]
                proportion = owner_forecast / individual_forecasts[month] if individual_forecasts[month] != 0 else 0
                adjustment = discrepancy[month] * proportion
                reconciled_forecast_results[owner]["reconciled_forecast"][month] += adjustment
    
    elif RECONCILIATION_METHOD == "weight_forecast":
        # Weight by forecast uncertainty (using confidence interval width)
        logger.info("Using uncertainty-weighted reconciliation method")
        print("Using uncertainty-weighted reconciliation method")
        
        for month in range(FORECAST_MONTHS):
            # Calculate weights based on forecast uncertainty
            weights = {}
            total_weight = 0
            
            for owner in other_owners:
                conf_int = initial_forecast_results[owner]["conf_int"]
                uncertainty = conf_int.iloc[month, 1] - conf_int.iloc[month, 0]  # Upper - Lower
                
                # Inverse of uncertainty as weight (more certain forecasts get higher weight)
                weight = 1.0 / uncertainty if uncertainty > 0 else 1.0
                weights[owner] = weight
                total_weight += weight
            
            # Normalize weights
            for owner in weights:
                weights[owner] /= total_weight
            
            # Apply weighted adjustments
            for owner in other_owners:
                if "reconciled_forecast" not in reconciled_forecast_results.get(owner, {}):
                    reconciled_forecast_results[owner] = {
                        "reconciled_forecast": initial_forecast_results[owner]["forecast"].copy(),
                        "future_dates": initial_forecast_results[owner]["future_dates"],
                        "model_type": initial_forecast_results[owner]["model_type"],
                        "initial_forecast": initial_forecast_results[owner]["forecast"].copy(),
                        "conf_int": initial_forecast_results[owner]["conf_int"]
                    }
                
                # Weighted adjustment
                adjustment = discrepancy[month] * weights[owner]
                reconciled_forecast_results[owner]["reconciled_forecast"][month] += adjustment
    
    elif RECONCILIATION_METHOD == "weight_history":
        # Weight by historical share
        logger.info("Using historical share-weighted reconciliation method")
        print("Using historical share-weighted reconciliation method")
        
        # Calculate historical share for each owner
        total_data = df[df['Owner'] == "Total"].sort_values('Date')
        historical_shares = {}
        
        for owner in other_owners:
            owner_data = df[df['Owner'] == owner].sort_values('Date')
            # Merge on date to ensure alignment
            merged_data = pd.merge(owner_data, total_data, on='Date', suffixes=('_owner', '_total'))
            # Calculate historical share
            merged_data['share'] = merged_data['Value_owner'] / merged_data['Value_total']
            # Use average share over the last year or all available data if less
            last_n = min(12, len(merged_data))
            avg_share = merged_data['share'].tail(last_n).mean()
            historical_shares[owner] = avg_share
        
        # Normalize shares to sum to 1
        share_sum = sum(historical_shares.values())
        for owner in historical_shares:
            historical_shares[owner] /= share_sum
        
        # Apply weighted adjustments
        for owner in other_owners:
            if "reconciled_forecast" not in reconciled_forecast_results.get(owner, {}):
                reconciled_forecast_results[owner] = {
                    "reconciled_forecast": initial_forecast_results[owner]["forecast"].copy(),
                    "future_dates": initial_forecast_results[owner]["future_dates"],
                    "model_type": initial_forecast_results[owner]["model_type"],
                    "initial_forecast": initial_forecast_results[owner]["forecast"].copy(),
                    "conf_int": initial_forecast_results[owner]["conf_int"]
                }
            
            # Apply historical share-based adjustment for each month
            for month in range(FORECAST_MONTHS):
                adjustment = discrepancy[month] * historical_shares[owner]
                reconciled_forecast_results[owner]["reconciled_forecast"][month] += adjustment
    
    else:
        logger.warning(f"Unknown reconciliation method: {RECONCILIATION_METHOD}. Using initial forecasts.")
        print(f"Unknown reconciliation method: {RECONCILIATION_METHOD}. Using initial forecasts.")
        reconciled_forecast_results = initial_forecast_results
    
    # Verify reconciliation
    reconciled_sum = np.zeros(FORECAST_MONTHS)
    for owner in other_owners:
        reconciled_sum += reconciled_forecast_results[owner]["reconciled_forecast"]
    
    # Display reconciliation results
    reconciliation_error = total_forecast - reconciled_sum
    print("\nReconciliation verification:")
    print(f"Total forecast: {total_forecast}")
    print(f"Sum of reconciled individual forecasts: {reconciled_sum}")
    print(f"Remaining discrepancy: {reconciliation_error}")
    print(f"Max absolute error: {np.max(np.abs(reconciliation_error))}")
    
    # Assign reconciled forecasts to final forecast results
    forecast_results = {}
    for owner in all_owners:
        if owner == "Total":
            forecast_results[owner] = initial_forecast_results[owner]
        else:
            forecast_results[owner] = {
                'forecast': reconciled_forecast_results[owner]["reconciled_forecast"],
                'initial_forecast': initial_forecast_results[owner]["forecast"],
                'conf_int': initial_forecast_results[owner]["conf_int"],
                'future_dates': initial_forecast_results[owner]["future_dates"],
                'model_type': initial_forecast_results[owner]["model_type"]
            }
else:
    # If no "Total" owner, use initial forecasts
    forecast_results = initial_forecast_results

# Create model comparison table
logger.info("Creating model comparison table")
model_comparison = []

for owner in all_owners:
    metrics_arima = arima_results[owner]['metrics'] if owner in arima_results and arima_results[owner] is not None else None
    metrics_linear = linear_results[owner]['metrics']
    
    # Choose the best model based on RMSE
    best_model = None
    if metrics_arima is not None:
        if metrics_arima['rmse'] < metrics_linear['rmse']:
            best_model = "ARIMA"
        else:
            best_model = "Linear"
    else:
        best_model = "Linear"
    
    model_comparison.append({
        'Owner': owner,
        'ARIMA_R2': metrics_arima['r2'] if metrics_arima else None,
        'ARIMA_RMSE': metrics_arima['rmse'] if metrics_arima else None,
        'ARIMA_MAE': metrics_arima['mae'] if metrics_arima else None,
        'ARIMA_MAPE': metrics_arima['mape'] if metrics_arima else None,
        'Linear_R2': metrics_linear['r2'],
        'Linear_RMSE': metrics_linear['rmse'],
        'Linear_MAE': metrics_linear['mae'],
        'Linear_MAPE': metrics_linear['mape'],
        'Best_Model': best_model,
        'Selected_Model': forecast_results[owner]['model_type']
    })

# Create and display model comparison DataFrame
model_comparison_df = pd.DataFrame(model_comparison)
print("\nModel Comparison:")
display(model_comparison_df)

# Plot forecasts for each owner
logger.info("Plotting forecasts for each owner")
for owner in all_owners:
    # Get owner data
    owner_data = df[df['Owner'] == owner].sort_values('Date')
    dates = owner_data['Date']
    values = owner_data['Value']
    
    # Get forecast data
    forecast = forecast_results[owner]['forecast']
    conf_int = forecast_results[owner]['conf_int']
    future_dates = forecast_results[owner]['future_dates']
    model_type = forecast_results[owner]['model_type']
    
    # Get metrics for the selected model
    if "ARIMA" in model_type:
        metrics = arima_results[owner]['metrics']
    else:
        metrics = linear_results[owner]['metrics']
    
    # Create plot
    plt.figure(figsize=FIGSIZE_TIMESERIES)
    
    # Plot historical data
    plt.plot(dates, values, label='Historical Data', marker='o', color='blue')
    
    # Plot forecast
    plt.plot(future_dates, forecast, label=f'Forecast ({model_type})', 
             color='red', marker='x', linestyle='--')
    
    # Plot initial forecast if reconciled
    if is_total_present and owner != "Total" and 'initial_forecast' in forecast_results[owner]:
        initial_forecast = forecast_results[owner]['initial_forecast']
        plt.plot(future_dates, initial_forecast, label=f'Initial Forecast (pre-reconciliation)', 
                 color='green', marker='+', linestyle=':')
    
    # Plot confidence interval
    if isinstance(conf_int, pd.DataFrame):
        plt.fill_between(future_dates, 
                         conf_int['lower'], 
                         conf_int['upper'], 
                         color='red', alpha=0.2)
    
    # Add labels and title
    title = f'Monthly Forecast for Owner: {owner} using {model_type}'
    if is_total_present and owner != "Total":
        title += f' (Reconciled using {RECONCILIATION_METHOD})'
    
    # Add metrics to title
    title += f'\nRÂ²: {metrics["r2"]:.3f}, RMSE: {metrics["rmse"]:.3f}, MAPE: {metrics["mape"]:.1f}%'
    
    plt.title(title)
    plt.xlabel('Date')
    plt.ylabel('Value')
    plt.grid(True)
    plt.legend()
    
    # Add vertical line at the forecast start
    plt.axvline(x=dates.iloc[-1], color='green', linestyle='--', 
                label='Forecast Start')
    
    # Format date axis to show month-year
    plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%b %Y'))
    plt.gcf().autofmt_xdate()
    
    # Display plot
    display(plt.gcf())
    plt.close()

# Create summary report
logger.info("Creating summary report")
summary_data = []

for owner in all_owners:
    # Get original data
    owner_data = df[df['Owner'] == owner]
    
    # Get forecast data
    forecast = forecast_results[owner]['forecast']
    model_type = forecast_results[owner]['model_type']
    future_dates = forecast_results[owner]['future_dates']
    
    # Get metrics for the selected model
    if "ARIMA" in model_type:
        metrics = arima_results[owner]['metrics']
    else:
        metrics = linear_results[owner]['metrics']
    
    # Get initial forecast if reconciled
    is_reconciled = is_total_present and owner != "Total" and 'initial_forecast' in forecast_results[owner]
    if is_reconciled:
        initial_forecast = forecast_results[owner]['initial_forecast']
    
    # Calculate growth metrics
    last_value = owner_data['Value'].iloc[-1]
    forecast_first = forecast[0]  # First month forecast
    forecast_third = forecast[2] if len(forecast) > 2 else np.nan  # 3-month forecast
    forecast_sixth = forecast[5] if len(forecast) > 5 else np.nan  # 6-month forecast
    forecast_last = forecast[-1]  # 12-month forecast
    
    # Calculate percentage changes
    pct_change_1m = ((forecast_first - last_value) / last_value) * 100 if last_value != 0 else np.inf
    pct_change_3m = ((forecast_third - last_value) / last_value) * 100 if last_value != 0 else np.inf
    pct_change_6m = ((forecast_sixth - last_value) / last_value) * 100 if last_value != 0 else np.inf
    pct_change_12m = ((forecast_last - last_value) / last_value) * 100 if last_value != 0 else np.inf
    
    # Prepare summary data
    summary_item = {
        'Owner': owner,
        'Model_Type': model_type,
        'Months_of_Data': len(owner_data),
        'R2': round(metrics['r2'], 3),
        'RMSE': round(metrics['rmse'], 3),
        'MAPE': round(metrics['mape'], 2),
        'Last_Actual_Value': round(last_value, 2),
        'Forecast_1M': round(forecast_first, 2),
        'Forecast_3M': round(forecast_third, 2) if not np.isnan(forecast_third) else np.nan,
        'Forecast_6M': round(forecast_sixth, 2) if not np.isnan(forecast_sixth) else np.nan,
        'Forecast_12M': round(forecast_last, 2),
        '%_Change_1M': round(pct_change_1m, 2),
        '%_Change_3M': round(pct_change_3m, 2) if not np.isnan(forecast_third) else np.nan,
        '%_Change_6M': round(pct_change_6m, 2) if not np.isnan(forecast_sixth) else np.nan,
        '%_Change_12M': round(pct_change_12m, 2)
    }
    
    # Add reconciliation info if applicable
    if is_reconciled:
        summary_item['Initial_Forecast_1M'] = round(initial_forecast[0], 2)
        summary_item['Initial_Forecast_12M'] = round(initial_forecast[-1], 2)
        summary_item['Reconciliation_Adjust_1M'] = round(forecast_first - initial_forecast[0], 2)
        summary_item['Reconciliation_Adjust_12M'] = round(forecast_last - initial_forecast[-1], 2)
    
    summary_data.append(summary_item)

# Create DataFrame and display summary
summary_df = pd.DataFrame(summary_data)
print("Forecast Summary Report:")
display(summary_df)

# Create detailed forecast tables for each owner
logger.info("Creating detailed forecast tables for each owner")
for owner in all_owners:
    # Get forecast data
    forecast = forecast_results[owner]['forecast']
    future_dates = forecast_results[owner]['future_dates']
    model_type = forecast_results[owner]['model_type']
    
    # Create DataFrame for forecasts
    forecast_data = {
        'Month': [d.strftime('%Y-%m') for d in future_dates],
        'Forecasted_Value': forecast,
        'Model_Type': model_type
    }
    
    # Add initial forecast if reconciled
    if is_total_present and owner != "Total" and 'initial_forecast' in forecast_results[owner]:
        forecast_data['Initial_Forecast'] = forecast_results[owner]['initial_forecast']
        forecast_data['Adjustment'] = forecast - forecast_results[owner]['initial_forecast']
    
    forecast_df = pd.DataFrame(forecast_data)
    
    # Display
    print(f"Detailed monthly forecast for {owner} using {model_type}:")
    display(forecast_df)

# Optional: Seasonal analysis for owners with sufficient data
logger.info("Performing seasonal analysis for owners with sufficient data")
for owner in all_owners:
    owner_data = df[df['Owner'] == owner].sort_values('Date')
    
    # Only analyze if we have enough data (at least 2 years)
    if len(owner_data) >= 24:
        logger.info(f"Performing seasonal analysis for Owner: {owner}")
        print(f"Seasonal analysis for Owner: {owner}")
        
        # Create a month column
        owner_data['Month'] = owner_data['Date'].dt.month
        
        # Group by month and calculate average
        monthly_avg = owner_data.groupby('Month')['Value'].mean().reset_index()
        monthly_avg['Month_Name'] = monthly_avg['Month'].apply(lambda x: datetime(2000, x, 1).strftime('%b'))
        
        # Plot monthly averages
        plt.figure(figsize=(12, 6))
        plt.bar(monthly_avg['Month_Name'], monthly_avg['Value'], color='skyblue')
        plt.title(f'Monthly Seasonal Pattern for Owner: {owner}')
        plt.xlabel('Month')
        plt.ylabel('Average Value')
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        
        # Display plot
        display(plt.gcf())
        plt.close()
        
        # Identify peak and trough months
        peak_month = monthly_avg.loc[monthly_avg['Value'].idxmax()]
        trough_month = monthly_avg.loc[monthly_avg['Value'].idxmin()]
        
        print(f"Peak month: {peak_month['Month_Name']} (Average: {peak_month['Value']:.2f})")
        print(f"Trough month: {trough_month['Month_Name']} (Average: {trough_month['Value']:.2f})")
        print("-" * 50)
        
        # Display monthly average data
        print("Monthly average values:")
        display(monthly_avg[['Month_Name', 'Value']].sort_values('Month'))

# Output model_metadata dictionary that can be used for MLflow tracking
logger.info("Model training and forecasting complete")
print("\nModel metadata for MLflow tracking:")
for owner, metadata in model_metadata.items():
    print(f"Owner: {owner}, Model Type: {metadata['model_type']}, Framework: {metadata['framework']}")

# Return the model_metadata dictionary for use in the MLflow tracking code block
model_metadata

# MLflow Model Registration and Tracking
import mlflow
import mlflow.sklearn
import mlflow.statsmodels
import pickle
import json
import os
from datetime import datetime

# This code block assumes model_metadata is available from the previous code block
# If model_metadata is not available, you can load it from a saved file or rerun the model training

# ====================================================
# MLflow PARAMETER SECTION - Modify these values as needed
# ====================================================
# MLflow tracking parameters
experiment_name = "Owner_Value_Forecasting"  # Change to your experiment name
run_name_prefix = "Monthly_Forecast_"        # Will be appended with date and owner
model_prefix = "owner_forecast_"             # Will be appended with owner name
model_stage = "Production"                   # Options: "None", "Staging", "Production", "Archived"

# Set this to the Model Registry URI (if using remote registry)
model_registry_uri = None  # Example: "databricks" or "http://your-mlflow-server:5000"

# Additional run tags
run_tags = {
    "model_type": "time_series",
    "data_frequency": "monthly",
    "forecast_horizon": f"{FORECAST_MONTHS}_months",
    "reconciliation_method": RECONCILIATION_METHOD if is_total_present else "none"
}

# Model details (for documentation)
model_details = {
    "description": "Monthly value forecasting by owner", 
    "min_months_for_arima": MIN_MONTHS_FOR_ARIMA,
    "forecast_months": FORECAST_MONTHS,
    "confidence_level": CONFIDENCE_LEVEL,
    "created_by": "Forecast_Pipeline_v1.0"
}
# ====================================================

# Set MLflow tracking URI if provided
if model_registry_uri:
    mlflow.set_tracking_uri(model_registry_uri)
    print(f"MLflow tracking URI set to: {model_registry_uri}")

# Create or get the experiment
try:
    experiment_id = mlflow.create_experiment(experiment_name)
    print(f"Created new experiment: {experiment_name} with ID: {experiment_id}")
except Exception:
    experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id
    print(f"Using existing experiment: {experiment_name} with ID: {experiment_id}")

mlflow.set_experiment(experiment_name)

# Function to register a model in MLflow
def register_model(owner, metadata, artifact_path):
    try:
        # Register the model
        model_name = f"{model_prefix}{owner.replace(' ', '_').lower()}"
        registered_model = mlflow.register_model(
            f"runs:/{run_id}/{artifact_path}",
            model_name
        )
        
        # Transition the model to the specified stage
        client = mlflow.tracking.MlflowClient()
        client.transition_model_version_stage(
            name=model_name,
            version=registered_model.version,
            stage=model_stage
        )
        
        # Add model description and details
        client.update_model_version(
            name=model_name,
            version=registered_model.version,
            description=json.dumps(model_details)
        )
        
        print(f"Model for owner '{owner}' registered as '{model_name}' " +
              f"version {registered_model.version} in stage '{model_stage}'")
        
        return registered_model
    except Exception as e:
        print(f"Error registering model for {owner}: {str(e)}")
        return None

# Log each model in a separate MLflow run
registered_models = {}

for owner, metadata in model_metadata.items():
    model_fit = metadata['model_fit']
    model_type = metadata['model_type']
    framework = metadata['framework']
    params = metadata['params']
    metrics = metadata['metrics']
    
    # Add reconciliation info to tags for non-Total owners
    owner_tags = run_tags.copy()
    if is_total_present and owner != "Total":
        owner_tags["reconciled"] = "true"
    elif owner == "Total":
        owner_tags["is_total"] = "true"
    
    # Start a new MLflow run for this owner
    run_name = f"{run_name_prefix}{owner.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    with mlflow.start_run(run_name=run_name) as run:
        run_id = run.info.run_id
        print(f"Started MLflow run: {run_name} with ID: {run_id}")
        
        # Log parameters
        mlflow.log_params({
            "owner": owner,
            "model_type": model_type,
            "framework": framework,
            "min_months_for_arima": MIN_MONTHS_FOR_ARIMA,
            "forecast_months": FORECAST_MONTHS,
            **params  # Unpack the model parameters
        })
        
        # Log tags
        mlflow.set_tags({**owner_tags, "owner": owner})
        
        # Log metrics from sklearn
        mlflow.log_metrics({
            "r2": metrics["r2"],
            "mse": metrics["mse"],
            "rmse": metrics["rmse"],
            "mae": metrics["mae"],
            "mape": metrics["mape"]
        })
        
        # Log model and additional metrics based on model type
        if "ARIMA" in model_type:
            # For ARIMA models
            artifact_path = "arima_model"
            mlflow.statsmodels.log_model(model_fit, artifact_path)
            
            # Log additional ARIMA-specific metrics
            aic = model_fit.aic if hasattr(model_fit, 'aic') else None
            bic = model_fit.bic if hasattr(model_fit, 'bic') else None
            if aic is not None:
                mlflow.log_metric("AIC", aic)
            if bic is not None:
                mlflow.log_metric("BIC", bic)
            
            # Register the model
            registered_model = register_model(owner, metadata, artifact_path)
            registered_models[owner] = registered_model
            
        else:
            # For Linear Regression models
            artifact_path = "linear_model"
            mlflow.sklearn.log_model(model_fit, artifact_path)
            
            # Register the model
            registered_model = register_model(owner, metadata, artifact_path)
            registered_models[owner] = registered_model
        
        # Log the forecast data as a CSV artifact
        forecast_data = pd.DataFrame({
            'Month': [d.strftime('%Y-%m') for d in forecast_results[owner]['future_dates']],
            'Forecasted_Value': forecast_results[owner]['forecast']
        })
        
        # Add initial forecast if reconciled
        if is_total_present and owner != "Total" and 'initial_forecast' in forecast_results[owner]:
            forecast_data['Initial_Forecast'] = forecast_results[owner]['initial_forecast']
            forecast_data['Adjustment'] = forecast_results[owner]['forecast'] - forecast_results[owner]['initial_forecast']
            
        forecast_path = f"{owner.replace(' ', '_')}_forecast.csv"
        forecast_data.to_csv(forecast_path, index=False)
        mlflow.log_artifact(forecast_path)
        os.remove(forecast_path)  # Clean up the temporary file
        
        # Create and log a forecast plot
        plt.figure(figsize=FIGSIZE_TIMESERIES)
        owner_data = df[df['Owner'] == owner].sort_values('Date')
        plt.plot(owner_data['Date'], owner_data['Value'], label='Historical Data', marker='o')
        plt.plot(forecast_results[owner]['future_dates'], forecast_results[owner]['forecast'], 
                 label=f'Forecast ({model_type})', color='red', linestyle='--', marker='x')
                 
        # Add initial forecast if reconciled
        if is_total_present and owner != "Total" and 'initial_forecast' in forecast_results[owner]:
            plt.plot(forecast_results[owner]['future_dates'], forecast_results[owner]['initial_forecast'], 
                     label='Initial Forecast', color='green', linestyle=':', marker='+')
        
        plt.title(f'Value Forecast for Owner: {owner}')
        plt.xlabel('Date')
        plt.ylabel('Value')
        plt.grid(True)
        plt.legend()
        plt.tight_layout()
        plot_path = f"{owner.replace(' ', '_')}_forecast_plot.png"
        plt.savefig(plot_path, dpi=300)
        mlflow.log_artifact(plot_path)
        plt.close()
        os.remove(plot_path)  # Clean up the temporary file
        
        # Log reconciliation info if applicable
        if is_total_present:
            if owner == "Total":
                # Log total's sum check
                reconciliation_sum = np.zeros(FORECAST_MONTHS)
                for other_owner in other_owners:
                    reconciliation_sum += forecast_results[other_owner]['forecast']
                
                reconciliation_error = forecast_results["Total"]['forecast'] - reconciliation_sum
                mlflow.log_metric("max_reconciliation_error", float(np.max(np.abs(reconciliation_error))))
                mlflow.log_metric("mean_reconciliation_error", float(np.mean(np.abs(reconciliation_error))))
            elif 'initial_forecast' in forecast_results[owner]:
                # Log adjustment metrics for individual owners
                initial = forecast_results[owner]['initial_forecast']
                reconciled = forecast_results[owner]['forecast']
                adjustment = reconciled - initial
                mlflow.log_metric("mean_adjustment", float(np.mean(adjustment)))
                mlflow.log_metric("max_adjustment", float(np.max(np.abs(adjustment))))
                mlflow.log_metric("pct_adjustment", float(np.mean(np.abs(adjustment) / np.abs(initial)) * 100))
        
        print(f"Completed MLflow run for owner: {owner}")

# Display registered models summary
print("\nRegistered Models Summary:")
registered_models_df = pd.DataFrame([
    {
        "Owner": owner,
        "Model Name": f"{model_prefix}{owner.replace(' ', '_').lower()}",
        "Model Type": model_metadata[owner]['model_type'],
        "R2": round(model_metadata[owner]['metrics']['r2'], 3),
        "RMSE": round(model_metadata[owner]['metrics']['rmse'], 3),
        "MAPE": round(model_metadata[owner]['metrics']['mape'], 2),
        "Version": registered_model.version if registered_model else "Registration Failed",
        "Stage": model_stage
    }
    for owner, registered_model in registered_models.items()
])
display(registered_models_df)

# Instructions for loading models from MLflow
print("\nTo load a specific model from MLflow, use the following code:")
print("```python")
print("# For ARIMA models")
print("import mlflow.statsmodels")
print("loaded_model = mlflow.statsmodels.load_model('runs:/<run_id>/arima_model')")
print("")
print("# For Linear Regression models")
print("import mlflow.sklearn")
print("loaded_model = mlflow.sklearn.load_model('runs:/<run_id>/linear_model')")
print("```")
