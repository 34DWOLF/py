# Import required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller, acf, pacf
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error
from datetime import datetime, timedelta
import warnings
from dateutil.relativedelta import relativedelta
import logging
from scipy.optimize import nnls

# ====================================================
# PARAMETER SECTION - Modify these values as needed
# ====================================================
# Minimum number of months required for ARIMA modeling
MIN_MONTHS_FOR_ARIMA = 6  # Set to allow ARIMA with 6 months of data

# Minimum number of months required for stationarity testing
MIN_MONTHS_FOR_STATIONARITY = 6  # Minimum needed

# Number of months to forecast
FORECAST_MONTHS = 3  # Forecast half the length of available data

# Maximum lags to consider for ACF/PACF analysis
MAX_LAGS_DEFAULT = 2  # Limited with 6 data points
MAX_LAGS_SEASONAL = 6  # Cannot detect yearly seasonality

# Confidence interval level (%)
CONFIDENCE_LEVEL = 90  # 90% confidence as requested
Z_VALUE = 1.645  # Corresponds to 90% confidence

# Non-negativity constraints
ENFORCE_NON_NEGATIVE = True  # Flag to ensure forecasts don't go below zero
LOG_TRANSFORM = True  # Use log transformation for non-negative forecasts
MIN_VALUE = 0.01  # Minimum value to use when log transforming (to avoid log(0))
CONSTRAINED_LINEAR = True  # Use constrained linear regression (non-negative)

# Reconciliation method
RECONCILIATION_METHOD = "proportional"  

# Plot settings
FIGSIZE_TIMESERIES = (14, 7)
FIGSIZE_ACF_PACF = (12, 8)
# ====================================================

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('forecast_model')

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

# Set plot style for better visualizations
plt.style.use('seaborn-whitegrid')

# Use the model_data DataFrame from previous code block instead of loading from CSV
# We assume that model_data is available in the current namespace
df = model_data.copy()
logger.info(f"Using model_data DataFrame with {len(df)} rows")

# Ensure Date is in datetime format
if not pd.api.types.is_datetime64_any_dtype(df['Date']):
    df['Date'] = pd.to_datetime(df['Date'])
    logger.info("Converted Date column to datetime format")

# Since data is monthly, ensure it's set to the first day of each month for consistency
df['Date'] = df['Date'].apply(lambda x: x.replace(day=1))
logger.info(f"Dates set to first day of each month for consistency")

# Display the first few rows
print("Data sample:")
display(df.head())

# Get basic statistics
print("\nData summary:")
display(df.describe())

# Check for missing values
print("\nMissing values:")
display(df.isnull().sum())

# Get unique owners
all_owners = df['Owner'].unique()
logger.info(f"Found {len(all_owners)} unique owners")

# Separate "Total" from other owners
is_total_present = "Total" in all_owners
if is_total_present:
    other_owners = [owner for owner in all_owners if owner != "Total"]
    logger.info(f"Found 'Total' owner and {len(other_owners)} other owners")
else:
    other_owners = all_owners
    logger.info("No 'Total' owner found, treating all owners individually")

print(f"\nNumber of unique owners: {len(all_owners)}")
print("Owners:", all_owners)

# Count data points per owner
owner_counts = df.groupby('Owner').size().reset_index(name='Count')
print("\nData points per owner:")
display(owner_counts)

# Helper function to convert numpy.datetime64 to Python datetime
def to_python_datetime(dt):
    if isinstance(dt, np.datetime64):
        return pd.Timestamp(dt).to_pydatetime()
    return dt

# Check date ranges per owner
date_ranges = []
for owner in all_owners:
    owner_data = df[df['Owner'] == owner].sort_values('Date')
    first_date = to_python_datetime(owner_data['Date'].min())
    last_date = to_python_datetime(owner_data['Date'].max())
    
    # Calculate months difference properly
    months_diff = (last_date.year - first_date.year) * 12 + last_date.month - first_date.month + 1
    
    date_ranges.append({
        'Owner': owner,
        'First Month': first_date.strftime('%Y-%m'),
        'Last Month': last_date.strftime('%Y-%m'),
        'Months Count': len(owner_data),
        'Expected Months': months_diff,
        'Is Continuous': len(owner_data) == months_diff
    })

date_range_df = pd.DataFrame(date_ranges)
print("\nDate ranges per owner:")
display(date_range_df)

# Define stationarity check function
def check_stationarity(timeseries, owner_name=None):
    """
    Test stationarity using Augmented Dickey-Fuller test
    """
    # Perform ADF test
    result = adfuller(timeseries)
    
    # Extract results
    dftest = pd.Series(
        result[0:4],
        index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used']
    )
    
    for key, value in result[4].items():
        dftest[f'Critical Value ({key})'] = value
    
    # Determine if stationary
    is_stationary = result[1] <= 0.05
    
    # Display results
    if owner_name:
        print(f"Results of Augmented Dickey-Fuller Test for Owner: {owner_name}")
    else:
        print("Results of Augmented Dickey-Fuller Test:")
    
    print(dftest)
    print(f"Is the series stationary? {'Yes' if is_stationary else 'No'}")
    print("-" * 50)
    
    return is_stationary, result[1]

# Plot time series for each owner
logger.info("Plotting time series for each owner")
plt.figure(figsize=(FIGSIZE_TIMESERIES[0], len(all_owners) * 4))

for i, owner in enumerate(all_owners):
    owner_data = df[df['Owner'] == owner].sort_values('Date')
    
    plt.subplot(len(all_owners), 1, i+1)
    plt.plot(owner_data['Date'], owner_data['Value'], marker='o', linestyle='-', label=f"Data Points: {len(owner_data)}")
    plt.title(f'Monthly Time Series for Owner: {owner}')
    plt.ylabel('Value')
    plt.grid(True)
    plt.legend()
    
    # Format x-axis to show month-year
    plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%b %Y'))
    plt.gcf().autofmt_xdate()

plt.tight_layout()
display(plt.gcf())
plt.close()

# Test stationarity for each owner
logger.info("Testing stationarity for each owner")
stationarity_results = {}

for owner in all_owners:
    owner_data = df[df['Owner'] == owner].sort_values('Date')
    values = owner_data['Value'].values
    
    # Only test if we have enough data points
    if len(values) >= MIN_MONTHS_FOR_STATIONARITY:
        logger.info(f"Testing stationarity for owner: {owner}")
        is_stationary, p_value = check_stationarity(values, owner)
        stationarity_results[owner] = {
            'is_stationary': is_stationary,
            'p_value': p_value,
            'data_points': len(values)
        }
    else:
        logger.warning(f"Owner {owner} has only {len(values)} months of data, not enough for stationarity test.")
        print(f"Owner {owner} has only {len(values)} months of data, not enough for stationarity test.")
        stationarity_results[owner] = {
            'is_stationary': None,
            'p_value': None,
            'data_points': len(values)
        }

# Function to analyze and plot ACF and PACF
def analyze_acf_pacf(timeseries, owner_name, max_lags=MAX_LAGS_DEFAULT):
    """
    Analyze and plot ACF and PACF to determine ARIMA order
    """
    logger.info(f"Analyzing ACF/PACF for owner: {owner_name}, max_lags: {max_lags}")
    
    # Compute ACF and PACF
    acf_values = acf(timeseries, nlags=max_lags, fft=True)
    pacf_values = pacf(timeseries, nlags=max_lags, method='ols')
    
    # Plot
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=FIGSIZE_ACF_PACF)
    
    # ACF plot
    plot_acf(timeseries, ax=ax1, lags=max_lags)
    ax1.set_title(f'Autocorrelation Function for Owner: {owner_name}')
    
    # PACF plot
    plot_pacf(timeseries, ax=ax2, lags=max_lags)
    ax2.set_title(f'Partial Autocorrelation Function for Owner: {owner_name}')
    
    plt.tight_layout()
    display(plt.gcf())
    plt.close()
    
    # Find significant lags (95% confidence)
    significance_level = 1.96/np.sqrt(len(timeseries))
    
    # Significant lags in ACF (for MA/q order)
    significant_acf = [i for i in range(1, max_lags + 1) if abs(acf_values[i]) > significance_level]
    
    # Significant lags in PACF (for AR/p order)
    significant_pacf = [i for i in range(1, max_lags + 1) if abs(pacf_values[i]) > significance_level]
    
    print(f"Owner: {owner_name}")
    print(f"Significant lags in ACF (suggests MA/q order): {significant_acf}")
    print(f"Significant lags in PACF (suggests AR/p order): {significant_pacf}")
    
    # Check for seasonality (12 months)
    if max_lags >= 12 and len(timeseries) >= 24:
        seasonal_lag = 12
        if seasonal_lag in significant_acf or seasonal_lag in significant_pacf:
            logger.info(f"Potential yearly seasonality detected at lag {seasonal_lag} for {owner_name}")
            print(f"Potential yearly seasonality detected at lag {seasonal_lag}")
    
    print("-" * 50)
    
    # Suggested order
    p = max(significant_pacf) if significant_pacf else 1
    q = max(significant_acf) if significant_acf else 1
    
    return p, q, acf_values, pacf_values

# Analyze ACF and PACF for each owner with sufficient data
logger.info("Analyzing ACF/PACF for each owner")
acf_pacf_results = {}

for owner in all_owners:
    owner_data = df[df['Owner'] == owner].sort_values('Date')
    values = owner_data['Value'].values
    
    # Only analyze if we have enough data points
    if len(values) >= MIN_MONTHS_FOR_ARIMA:
        print(f"Analyzing ACF/PACF for Owner: {owner} (Months of data: {len(values)})")
        
        # Check if differencing is needed
        is_stationary, _ = stationarity_results[owner]['is_stationary'], stationarity_results[owner]['p_value']
        
        # If not stationary, difference the series
        if not is_stationary:
            logger.info(f"Series for {owner} is not stationary, applying differencing")
            print(f"Series for {owner} is not stationary, applying differencing")
            values_diff = np.diff(values)
            p, q, acf_vals, pacf_vals = analyze_acf_pacf(values_diff, f"{owner} (differenced)")
            d = 1
        else:
            p, q, acf_vals, pacf_vals = analyze_acf_pacf(values, owner)
            d = 0
            
        # For monthly data, we might need higher max_lags to catch seasonality
        if len(values) >= 36:  # If we have at least 3 years of data
            logger.info(f"Checking for seasonality with additional lags for {owner}")
            p_seasonal, q_seasonal, _, _ = analyze_acf_pacf(values if is_stationary else values_diff, 
                                                         f"{owner} (seasonal check)", 
                                                         max_lags=MAX_LAGS_SEASONAL)
            # Use the higher of the two if seasonal analysis found something
            p = max(p, p_seasonal) if p_seasonal > p else p
            q = max(q, q_seasonal) if q_seasonal > q else q
            
        acf_pacf_results[owner] = {
            'p': p,
            'd': d,
            'q': q,
            'acf_values': acf_vals,
            'pacf_values': pacf_vals
        }
        
        logger.info(f"Suggested ARIMA order for {owner}: ({p}, {d}, {q})")
        print(f"Suggested ARIMA order for {owner}: ({p}, {d}, {q})")
    else:
        logger.warning(f"Owner {owner} has only {len(values)} months of data, not enough for robust ACF/PACF analysis.")
        print(f"Owner {owner} has only {len(values)} months of data, not enough for robust ACF/PACF analysis.")
        acf_pacf_results[owner] = None

# Function to calculate evaluation metrics
def calculate_metrics(y_true, y_pred):
    """Calculate various evaluation metrics using sklearn"""
    metrics = {}
    
    # R-squared (coefficient of determination)
    metrics['r2'] = r2_score(y_true, y_pred)
    
    # Mean Squared Error
    metrics['mse'] = mean_squared_error(y_true, y_pred)
    
    # Root Mean Squared Error
    metrics['rmse'] = np.sqrt(metrics['mse'])
    
    # Mean Absolute Error
    metrics['mae'] = mean_absolute_error(y_true, y_pred)
    
    # Mean Absolute Percentage Error
    # Handle case where y_true contains zeros
    if np.any(y_true == 0):
        # Calculate MAPE only for non-zero values
        non_zero_indices = y_true != 0
        if np.any(non_zero_indices):
            metrics['mape'] = mean_absolute_percentage_error(
                y_true[non_zero_indices], 
                y_pred[non_zero_indices]
            ) * 100  # Convert to percentage
        else:
            metrics['mape'] = np.nan
    else:
        metrics['mape'] = mean_absolute_percentage_error(y_true, y_pred) * 100  # Convert to percentage
    
    return metrics

# Train ARIMA models for owners with sufficient data
logger.info("Training ARIMA models for eligible owners")
arima_results = {}

for owner in all_owners:
    owner_data = df[df['Owner'] == owner].sort_values('Date')
    values = owner_data['Value'].values
    dates = owner_data['Date'].values
    
    # Only attempt ARIMA if we have enough data points
    if len(values) >= MIN_MONTHS_FOR_ARIMA:
        logger.info(f"Training ARIMA model for Owner: {owner}")
        print(f"Training ARIMA model for Owner: {owner}")
        
        # Get suggested order from previous analysis
        if owner in acf_pacf_results and acf_pacf_results[owner] is not None:
            p, d, q = acf_pacf_results[owner]['p'], acf_pacf_results[owner]['d'], acf_pacf_results[owner]['q']
        else:
            # Simple default if not available
            p, d, q = 1, 0, 0
        
        try:
            # Apply transformation if needed for non-negative forecasts
            if LOG_TRANSFORM and ENFORCE_NON_NEGATIVE:
                # Check for non-positive values
                min_value = np.min(values)
                if min_value <= 0:
                    # Add offset to ensure all values are positive
                    transformed_values = np.log(values + MIN_VALUE)
                    logger.info(f"Applied log transformation with offset {MIN_VALUE} for {owner}")
                else:
                    transformed_values = np.log(values)
                    logger.info(f"Applied log transformation for {owner}")
                
                # Train ARIMA on transformed data
                model = ARIMA(transformed_values, order=(p, d, q))
                model_fit = model.fit()
                
                # For in-sample predictions, we need to back-transform
                if d > 0:
                    # For differenced models
                    in_sample_pred_transformed = model_fit.predict(start=d, end=len(transformed_values)-1)
                    # Ensure lengths match for metric calculation
                    actuals = values[d:]
                    # Back-transform
                    if min_value <= 0:
                        in_sample_pred = np.exp(in_sample_pred_transformed) - MIN_VALUE
                    else:
                        in_sample_pred = np.exp(in_sample_pred_transformed)
                else:
                    # For non-differenced models
                    in_sample_pred_transformed = model_fit.predict(start=0, end=len(transformed_values)-1)
                    actuals = values
                    # Back-transform
                    if min_value <= 0:
                        in_sample_pred = np.exp(in_sample_pred_transformed) - MIN_VALUE
                    else:
                        in_sample_pred = np.exp(in_sample_pred_transformed)
                
                # Ensure non-negative predictions
                in_sample_pred = np.maximum(in_sample_pred, 0)
            else:
                # Standard ARIMA without transformation
                model = ARIMA(values, order=(p, d, q))
                model_fit = model.fit()
                
                # Calculate in-sample predictions
                if d > 0:
                    # For differenced models
                    in_sample_pred = model_fit.predict(start=d, end=len(values)-1)
                    # Ensure lengths match for metric calculation
                    actuals = values[d:]
                else:
                    in_sample_pred = model_fit.predict(start=0, end=len(values)-1)
                    actuals = values
                
                # Ensure non-negative predictions if required
                if ENFORCE_NON_NEGATIVE:
                    in_sample_pred = np.maximum(in_sample_pred, 0)
            
            # Calculate metrics
            metrics = calculate_metrics(actuals, in_sample_pred)
            
            # Store ARIMA results
            arima_results[owner] = {
                'model_fit': model_fit,
                'order': (p, d, q),
                'values': values,
                'transformed': LOG_TRANSFORM and ENFORCE_NON_NEGATIVE,
                'min_value': min_value if LOG_TRANSFORM and ENFORCE_NON_NEGATIVE and min_value <= 0 else 0,
                'dates': dates,
                'model_type': f"ARIMA({p},{d},{q})",
                'in_sample_pred': in_sample_pred,
                'metrics': metrics
            }
            
            # Print metrics
            print(f"ARIMA model metrics for {owner}:")
            for metric_name, metric_value in metrics.items():
                print(f"  {metric_name.upper()}: {metric_value:.4f}")
            
            logger.info(f"Successfully trained ARIMA({p},{d},{q}) for {owner}")
            
        except Exception as e:
            logger.warning(f"Error training ARIMA for {owner}: {str(e)}")
            print(f"Error training ARIMA for {owner}: {str(e)}")
            arima_results[owner] = None
    else:
        logger.info(f"Owner {owner} has insufficient data for ARIMA modeling.")
        print(f"Owner {owner} has insufficient data for ARIMA modeling.")
        arima_results[owner] = None

# Train linear regression models for all owners
logger.info("Training Linear Regression models for all owners")
linear_results = {}

for owner in all_owners:
    owner_data = df[df['Owner'] == owner].sort_values('Date')
    values = owner_data['Value'].values
    dates = owner_data['Date'].to_list()  # Convert to Python datetime objects
    
    logger.info(f"Training Linear Regression model for Owner: {owner}")
    print(f"Training Linear Regression model for Owner: {owner}")
    
    # For monthly data, convert dates to numeric feature (months since first date)
    first_date = min(dates)
    # Properly calculate months since first date
    X = np.array([
        (to_python_datetime(d).year - to_python_datetime(first_date).year) * 12 + 
        to_python_datetime(d).month - to_python_datetime(first_date).month 
        for d in dates
    ]).reshape(-1, 1)
    
    # Check if we should use constrained linear regression
    if CONSTRAINED_LINEAR and ENFORCE_NON_NEGATIVE:
        # Use Non-Negative Least Squares for constrained regression
        X_with_const = np.column_stack((np.ones(X.shape[0]), X))
        coeffs, _ = nnls(X_with_const, values)
        
        # Create a LinearRegression-like object
        model = LinearRegression()
        model.intercept_ = coeffs[0]
        model.coef_ = np.array([coeffs[1]])
        
        # Generate in-sample predictions
        in_sample_pred = model.intercept_ + X * model.coef_[0]
    else:
        # Standard linear regression
        model = LinearRegression()
        model.fit(X, values)
        
        # Generate in-sample predictions
        in_sample_pred = model.predict(X)
        
        # Ensure non-negative predictions if required
        if ENFORCE_NON_NEGATIVE:
            in_sample_pred = np.maximum(in_sample_pred, 0)
    
    # Calculate metrics
    metrics = calculate_metrics(values, in_sample_pred)
    
    # Print coefficient details
    print(f"Linear Regression coefficients for {owner}:")
    print(f"  Intercept: {model.intercept_:.4f}")
    print(f"  Slope (per month): {model.coef_[0]:.4f}")
    
    # Print metrics
    print(f"Linear Regression model metrics for {owner}:")
    for metric_name, metric_value in metrics.items():
        print(f"  {metric_name.upper()}: {metric_value:.4f}")
    
    # Store linear regression results
    linear_results[owner] = {
        'model_fit': model,
        'first_date': first_date,
        'values': values,
        'dates': dates,
        'in_sample_pred': in_sample_pred,
        'metrics': metrics,
        'model_type': "Linear Regression"
    }
    logger.info(f"Successfully trained Linear Regression for {owner}, R-squared: {metrics['r2']:.4f}")

# Generate initial forecasts for each owner
logger.info(f"Generating {FORECAST_MONTHS}-month initial forecasts for each owner")
initial_forecast_results = {}

# Store model metadata for MLflow tracking
model_metadata = {}

for owner in all_owners:
    logger.info(f"Generating initial forecast for Owner: {owner}")
    print(f"Generating initial forecast for Owner: {owner}")
    
    # Compare metrics to choose the best model
    if owner in arima_results and arima_results[owner] is not None:
        # Compare metrics to choose the best model
        arima_metrics = arima_results[owner]['metrics']
        linear_metrics = linear_results[owner]['metrics']
        
        if arima_metrics['rmse'] < linear_metrics['rmse']:
            use_arima = True
        else:
            use_arima = False
    else:
        use_arima = False
    
    # Check if we have a valid ARIMA model and should use it
    if use_arima:
        logger.info(f"Using {arima_results[owner]['model_type']} model for {owner}")
        print(f"Using {arima_results[owner]['model_type']} model for {owner}")
        
        # Get the model and data
        model_fit = arima_results[owner]['model_fit']
        values = arima_results[owner]['values']
        dates = arima_results[owner]['dates']
        model_type = arima_results[owner]['model_type']
        metrics = arima_results[owner]['metrics']
        is_transformed = arima_results[owner].get('transformed', False)
        min_value = arima_results[owner].get('min_value', 0)
        
        # Store model metadata
        model_metadata[owner] = {
            'model_fit': model_fit,
            'model_type': model_type,
            'framework': 'statsmodels.ARIMA',
            'metrics': metrics,
            'params': {
                'order': arima_results[owner]['order']
            }
        }
        
        # Generate forecast
        if is_transformed:
            # Generate forecast on transformed scale
            forecast_transformed = model_fit.forecast(steps=FORECAST_MONTHS)
            
            # Back-transform to original scale
            if min_value > 0:
                forecast = np.exp(forecast_transformed)
            else:
                forecast = np.exp(forecast_transformed) - MIN_VALUE
            
            # Ensure non-negative values
            forecast = np.maximum(forecast, 0)
            
            # Generate confidence intervals on transformed scale
            forecast_conf = model_fit.get_forecast(steps=FORECAST_MONTHS)
            conf_int_transformed = forecast_conf.conf_int(alpha=(1 - CONFIDENCE_LEVEL/100))
            
            # Back-transform confidence intervals
            if min_value > 0:
                conf_lower = np.exp(conf_int_transformed.iloc[:, 0])
                conf_upper = np.exp(conf_int_transformed.iloc[:, 1])
            else:
                conf_lower = np.exp(conf_int_transformed.iloc[:, 0]) - MIN_VALUE
                conf_upper = np.exp(conf_int_transformed.iloc[:, 1]) - MIN_VALUE
            
            # Ensure lower bound is non-negative
            conf_lower = np.maximum(conf_lower, 0)
            
            # Create a DataFrame for confidence intervals
            conf_int = pd.DataFrame({
                'lower': conf_lower,
                'upper': conf_upper
            })
        else:
            # Generate forecast directly
            forecast = model_fit.forecast(steps=FORECAST_MONTHS)
            
            # Ensure non-negative values if required
            if ENFORCE_NON_NEGATIVE:
                forecast = np.maximum(forecast, 0)
            
            # Generate confidence intervals
            forecast_conf = model_fit.get_forecast(steps=FORECAST_MONTHS)
            conf_int = forecast_conf.conf_int(alpha=(1 - CONFIDENCE_LEVEL/100))
            
            # Ensure lower bound is non-negative if required
            if ENFORCE_NON_NEGATIVE:
                conf_int.iloc[:, 0] = np.maximum(conf_int.iloc[:, 0], 0)
        
        # Generate future dates (monthly)
        last_date = to_python_datetime(dates[-1])
        future_dates = [last_date + relativedelta(months=i+1) for i in range(FORECAST_MONTHS)]
        
        # Store forecast results
        initial_forecast_results[owner] = {
            'forecast': forecast,
            'conf_int': conf_int,
            'future_dates': future_dates,
            'model_type': model_type
        }
    else:
        logger.info(f"Using Linear Regression model for {owner}")
        print(f"Using Linear Regression model for {owner}")
        
        # Get the model and data
        model_fit = linear_results[owner]['model_fit']
        first_date = linear_results[owner]['first_date']
        values = linear_results[owner]['values']
        dates = linear_results[owner]['dates']
        metrics = linear_results[owner]['metrics']
        
        # Store model metadata
        model_metadata[owner] = {
            'model_fit': model_fit,
            'model_type': 'Linear Regression',
            'framework': 'sklearn.LinearRegression',
            'metrics': metrics,
            'params': {
                'fit_intercept': True,
                'constrained': CONSTRAINED_LINEAR and ENFORCE_NON_NEGATIVE
            }
        }
        
        # Get the last date
        last_date = to_python_datetime(dates[-1])
        
        # Generate future months (as X values)
        months_since_first = [
            ((last_date.year - to_python_datetime(first_date).year) * 12 + 
             last_date.month - to_python_datetime(first_date).month) + i + 1 
            for i in range(FORECAST_MONTHS)
        ]
        future_X = np.array(months_since_first).reshape(-1, 1)
        
        # Generate forecast
        forecast = model_fit.predict(future_X)
        
        # Ensure non-negative values if required
        if ENFORCE_NON_NEGATIVE:
            forecast = np.maximum(forecast, 0)
        
        # Generate simple confidence intervals
        prediction_std = np.std(values - linear_results[owner]['in_sample_pred'])
        conf_lower = forecast - Z_VALUE * prediction_std
        conf_upper = forecast + Z_VALUE * prediction_std
        
        # Ensure lower bound is non-negative if required
        if ENFORCE_NON_NEGATIVE:
            conf_lower = np.maximum(conf_lower, 0)
        
        # Create a DataFrame similar to ARIMA's conf_int
        conf_int = pd.DataFrame({
            'lower': conf_lower,
            'upper': conf_upper
        })
        
        # Generate future dates (monthly)
        future_dates = [last_date + relativedelta(months=i+1) for i in range(FORECAST_MONTHS)]
        
        # Store forecast results
        initial_forecast_results[owner] = {
            'forecast': forecast,
            'conf_int': conf_int,
            'future_dates': future_dates,
            'model_type': "Linear Regression"
        }

# Reconcile forecasts if 'Total' is present
if is_total_present:
    logger.info("Reconciling forecasts to ensure consistency with 'Total'")
    print("\nReconciling forecasts to ensure consistency with 'Total'...")
    
    # Get total forecast
    total_forecast = initial_forecast_results["Total"]["forecast"]
    
    # Get sum of individual forecasts
    individual_forecasts = np.zeros(FORECAST_MONTHS)
    for owner in other_owners:
        individual_forecasts += initial_forecast_results[owner]["forecast"]
    
    # Calculate discrepancy
    discrepancy = total_forecast - individual_forecasts
    print(f"Discrepancy between 'Total' and sum of individuals: {discrepancy}")
    
    # Reconciliation methods
    reconciled_forecast_results = {}
    reconciled_forecast_results["Total"] = initial_forecast_results["Total"]  # Keep original Total
    
    if RECONCILIATION_METHOD == "proportional":
        # Proportional reconciliation
        logger.info("Using proportional reconciliation method")
        print("Using proportional reconciliation method")
        
        for month in range(FORECAST_MONTHS):
            # Skip reconciliation if sum is close to zero to avoid division by zero
            if abs(individual_forecasts[month]) < 1e-10:
                for owner in other_owners:
                    if "reconciled_forecast" not in reconciled_forecast_results.get(owner, {}):
                        reconciled_forecast_results[owner] = {
                            "reconciled_forecast": initial_forecast_results[owner]["forecast"].copy(),
                            "future_dates": initial_forecast_results[owner]["future_dates"],
                            "model_type": initial_forecast_results[owner]["model_type"],
                            "initial_forecast": initial_forecast_results[owner]["forecast"].copy(),
                            "conf_int": initial_forecast_results[owner]["conf_int"]
                        }
                continue
            
            # Calculate proportional adjustment for each owner
            for owner in other_owners:
                if "reconciled_forecast" not in reconciled_forecast_results.get(owner, {}):
                    reconciled_forecast_results[owner] = {
                        "reconciled_forecast": initial_forecast_results[owner]["forecast"].copy(),
                        "future_dates": initial_forecast_results[owner]["future_dates"],
                        "model_type": initial_forecast_results[owner]["model_type"],
                        "initial_forecast": initial_forecast_results[owner]["forecast"].copy(),
                        "conf_int": initial_forecast_results[owner]["conf_int"]
                    }
                
                # Proportional adjustment
                owner_forecast = initial_forecast_results[owner]["forecast"][month]
                proportion = owner_forecast / individual_forecasts[month] if individual_forecasts[month] != 0 else 0
                adjustment = discrepancy[month] * proportion
                reconciled_forecast_results[owner]["reconciled_forecast"][month] += adjustment
                
                # Ensure non-negative values after reconciliation if required
                if ENFORCE_NON_NEGATIVE:
                    reconciled_forecast_results[owner]["reconciled_forecast"][month] = max(
                        reconciled_forecast_results[owner]["reconciled_forecast"][month], 0
                    )
    
    elif RECONCILIATION_METHOD == "weight_forecast":
        # Weight by forecast uncertainty (using confidence interval width)
        logger.info("Using uncertainty-weighted reconciliation method")
        print("Using uncertainty-weighted reconciliation method")
        
        for month in range(FORECAST_MONTHS):
            # Calculate weights based on forecast uncertainty
            weights = {}
            total_weight = 0
            
            for owner in other_owners:
                conf_int = initial_forecast_results[owner]["conf_int"]
                uncertainty = conf_int.iloc[month, 1] - conf_int.iloc[month, 0]  # Upper - Lower
                
                # Inverse of uncertainty as weight (more certain forecasts get higher weight)
                weight = 1.0 / uncertainty if uncertainty > 0 else 1.0
                weights[owner] = weight
                total_weight += weight
            
            # Normalize weights
            for owner in weights:
                weights[owner] /= total_weight
            
            # Apply weighted adjustments
            for owner in other_owners:
                if "reconciled_forecast" not in reconciled_forecast_results.get(owner, {}):
                    reconciled_forecast_results[owner] = {
                        "reconciled_forecast": initial_forecast_results[owner]["forecast"].copy(),
                        "future_dates": initial_forecast_results[owner]["future_dates"],
                        "model_type": initial_forecast_results[owner]["model_type"],
                        "initial_forecast": initial_forecast_results[owner]["forecast"].copy(),
                        "conf_int": initial_forecast_results[owner]["conf_int"]
                    }
                
                # Weighted adjustment
                adjustment = discrepancy[month] * weights[owner]
                reconciled_forecast_results[owner]["reconciled_forecast"][month] += adjustment
                
                # Ensure non-negative values after reconciliation if required
                if ENFORCE_NON_NEGATIVE:
                    reconciled_forecast_results[owner]["reconciled_forecast"][month] = max(
                        reconciled_forecast_results[owner]["reconciled_forecast"][month], 0
                    )
    
    elif RECONCILIATION_METHOD == "weight_history":
        # Weight by historical share
        logger.info("Using historical share-weighted reconciliation method")
        print("Using historical share-weighted reconciliation method")
        
        # Calculate historical share for each owner
        total_data = df[df['Owner'] == "Total"].sort_values('Date')
        historical_shares = {}
        
        for owner in other_owners:
            owner_data = df[df['Owner'] == owner].sort_values('Date')
            # Merge on date to ensure alignment
            merged_data = pd.merge(owner_data, total_data, on='Date', suffixes=('_owner', '_total'))
            # Calculate historical share
            merged_data['share'] = merged_data['Value_owner'] / merged_data['Value_total']
            # Use average share over all available data (since we only have 6 months)
            avg_share = merged_data['share'].mean()
            historical_shares[owner] = avg_share
        
        # Normalize shares to sum to 1
        share_sum = sum(historical_shares.values())
        for owner in historical_shares:
            historical_shares[owner] /= share_sum
        
        # Apply weighted adjustments
        for owner in other_owners:
            if "reconciled_forecast" not in reconciled_forecast_results.get(owner, {}):
                reconciled_forecast_results[owner] = {
                    "reconciled_forecast": initial_forecast_results[owner]["forecast"].copy(),
                    "future_dates": initial_forecast_results[owner]["future_dates"],
                    "model_type": initial_forecast_results[owner]["model_type"],
                    "initial_forecast": initial_forecast_results[owner]["forecast"].copy(),
                    "conf_int": initial_forecast_results[owner]["conf_int"]
                }
            
            # Apply historical share-based adjustment for each month
            for month in range(FORECAST_MONTHS):
                adjustment = discrepancy[month] * historical_shares[owner]
                reconciled_forecast_results[owner]["reconciled_forecast"][month] += adjustment
                
                # Ensure non-negative values after reconciliation if required
                if ENFORCE_NON_NEGATIVE:
                    reconciled_forecast_results[owner]["reconciled_forecast"][month] = max(
                        reconciled_forecast_results[owner]["reconciled_forecast"][month], 0
                    )
    
    else:
        logger.warning(f"Unknown reconciliation method: {RECONCILIATION_METHOD}. Using initial forecasts.")
        print(f"Unknown reconciliation method: {RECONCILIATION_METHOD}. Using initial forecasts.")
        reconciled_forecast_results = initial_forecast_results
    
    # Verify reconciliation
    reconciled_sum = np.zeros(FORECAST_MONTHS)
    for owner in other_owners:
        reconciled_sum += reconciled_forecast_results[owner]["reconciled_forecast"]
    
    # Display reconciliation results
    reconciliation_error = total_forecast - reconciled_sum
    print("\nReconciliation verification:")
    print(f"Total forecast: {total_forecast}")
    print(f"Sum of reconciled individual forecasts: {reconciled_sum}")
    print(f"Remaining discrepancy: {reconciliation_error}")
    print(f"Max absolute error: {np.max(np.abs(reconciliation_error))}")
    
    # Assign reconciled forecasts to final forecast results
    forecast_results = {}
    for owner in all_owners:
        if owner == "Total":
            forecast_results[owner] = initial_forecast_results[owner]
        else:
            forecast_results[owner] = {
                'forecast': reconciled_forecast_results[owner]["reconciled_forecast"],
                'initial_forecast': initial_forecast_results[owner]["forecast"],
                'conf_int': initial_forecast_results[owner]["conf_int"],
                'future_dates': initial_forecast_results[owner]["future_dates"],
                'model_type': initial_forecast_results[owner]["model_type"]
            }
else:
    # If no "Total" owner, use initial forecasts
    forecast_results = initial_forecast_results

# Create model comparison table
logger.info("Creating model comparison table")
model_comparison = []

for owner in all_owners:
    metrics_arima = arima_results[owner]['metrics'] if owner in arima_results and arima_results[owner] is not None else None
    metrics_linear = linear_results[owner]['metrics']
    
    # Choose the best model based on RMSE
    best_model = None
    if metrics_arima is not None:
        if metrics_arima['rmse'] < metrics_linear['rmse']:
            best_model = "ARIMA"
        else:
            best_model = "Linear"
    else:
        best_model = "Linear"
    
    model_comparison.append({
        'Owner': owner,
        'ARIMA_R2': metrics_arima['r2'] if metrics_arima else None,
        'ARIMA_RMSE': metrics_arima['rmse'] if metrics_arima else None,
        'ARIMA_MAE': metrics_arima['mae'] if metrics_arima else None,
        'ARIMA_MAPE': metrics_arima['mape'] if metrics_arima else None,
        'Linear_R2': metrics_linear['r2'],
        'Linear_RMSE': metrics_linear['rmse'],
        'Linear_MAE': metrics_linear['mae'],
        'Linear_MAPE': metrics_linear['mape'],
        'Best_Model': best_model,
        'Selected_Model': forecast_results[owner]['model_type']
    })

# Create and display model comparison DataFrame
model_comparison_df = pd.DataFrame(model_comparison)
print("\nModel Comparison:")
display(model_comparison_df)

# Create forecast summary report
summary_data = []

for owner in all_owners:
    # Get original data
    owner_data = df[df['Owner'] == owner]
    
    # Get forecast data
    forecast = forecast_results[owner]['forecast']
    model_type = forecast_results[owner]['model_type']
    future_dates = forecast_results[owner]['future_dates']
    
    # Get metrics for the selected model
    if "ARIMA" in model_type:
        metrics = arima_results[owner]['metrics']
    else:
        metrics = linear_results[owner]['metrics']
    
    # Get initial forecast if reconciled
    is_reconciled = is_total_present and owner != "Total" and 'initial_forecast' in forecast_results[owner]
    if is_reconciled:
        initial_forecast = forecast_results[owner]['initial_forecast']
    
    # Calculate growth metrics
    last_value = owner_data['Value'].iloc[-1]
    forecast_first = forecast[0]  # First month forecast
    forecast_third = forecast[2] if len(forecast) > 2 else np.nan  # 3-month forecast
    forecast_last = forecast[-1]  # Last month forecast
    
    # Calculate percentage changes
    pct_change_1m = ((forecast_first - last_value) / last_value) * 100 if last_value != 0 else np.inf
    pct_change_3m = ((forecast_third - last_value) / last_value) * 100 if last_value != 0 else np.inf
    pct_change_last = ((forecast_last - last_value) / last_value) * 100 if last_value != 0 else np.inf
    
    # Prepare summary data
    summary_item = {
        'Owner': owner,
        'Model_Type': model_type,
        'Months_of_Data': len(owner_data),
        'R2': round(metrics['r2'], 3),
        'RMSE': round(metrics['rmse'], 3),
        'MAPE': round(metrics['mape'], 2),
        'Last_Actual_Value': round(last_value, 2),
        'Forecast_1M': round(forecast_first, 2),
        'Forecast_3M': round(forecast_third, 2) if not np.isnan(forecast_third) else np.nan,
        'Forecast_Last': round(forecast_last, 2),
        '%_Change_1M': round(pct_change_1m, 2),
        '%_Change_3M': round(pct_change_3m, 2) if not np.isnan(forecast_third) else np.nan,
        '%_Change_Last': round(pct_change_last, 2)
    }
    
    # Add reconciliation info if applicable
    if is_reconciled:
        summary_item['Initial_Forecast_1M'] = round(initial_forecast[0], 2)
        summary_item['Initial_Forecast_Last'] = round(initial_forecast[-1], 2)
        summary_item['Reconciliation_Adjust_1M'] = round(forecast_first - initial_forecast[0], 2)
        summary_item['Reconciliation_Adjust_Last'] = round(forecast_last - initial_forecast[-1], 2)
    
    summary_data.append(summary_item)

# Create DataFrame and display summary
summary_df = pd.DataFrame(summary_data)
print("Forecast Summary Report:")
display(summary_df)

# Create detailed forecast tables for each owner
detailed_forecast_data = []

for owner in all_owners:
    # Get forecast data
    forecast = forecast_results[owner]['forecast']
    future_dates = forecast_results[owner]['future_dates']
    model_type = forecast_results[owner]['model_type']
    
    # Add each month's forecast to the detailed data
    for i, (date, value) in enumerate(zip(future_dates, forecast)):
        forecast_row = {
            'Owner': owner,
            'Month': date.strftime('%Y-%m'),
            'Forecast': round(value, 2),
            'Model': model_type
        }
        
        # Add initial forecast if reconciled
        if is_total_present and owner != "Total" and 'initial_forecast' in forecast_results[owner]:
            initial_value = forecast_results[owner]['initial_forecast'][i]
            adjustment = value - initial_value
            forecast_row['Initial_Forecast'] = round(initial_value, 2)
            forecast_row['Adjustment'] = round(adjustment, 2)
        
        detailed_forecast_data.append(forecast_row)

# Create the detailed forecast DataFrame
detailed_forecast_df = pd.DataFrame(detailed_forecast_data)
print("\nDetailed Forecast by Month:")
display(detailed_forecast_df)
